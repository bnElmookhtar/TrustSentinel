{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "389339be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 654 images belonging to 2 classes.\n",
      "Found 34 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eman\\AppData\\Local\\Temp\\ipykernel_13288\\1503087569.py:38: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "Epoch 1/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813ms/step - accuracy: 0.5443 - loss: 0.6921"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 972ms/step - accuracy: 0.5449 - loss: 0.6917 - val_accuracy: 0.7059 - val_loss: 0.6009\n",
      "Epoch 2/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863ms/step - accuracy: 0.6070 - loss: 0.6542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 961ms/step - accuracy: 0.6081 - loss: 0.6534 - val_accuracy: 0.7941 - val_loss: 0.5595\n",
      "Epoch 3/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 896ms/step - accuracy: 0.6634 - loss: 0.6115 - val_accuracy: 0.7941 - val_loss: 0.5615\n",
      "Epoch 4/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 947ms/step - accuracy: 0.7286 - loss: 0.5824 - val_accuracy: 0.7941 - val_loss: 0.5138\n",
      "Epoch 5/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 922ms/step - accuracy: 0.7332 - loss: 0.5655 - val_accuracy: 0.7941 - val_loss: 0.4753\n",
      "Epoch 6/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 864ms/step - accuracy: 0.6969 - loss: 0.6014 - val_accuracy: 0.7941 - val_loss: 0.4683\n",
      "Epoch 7/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 932ms/step - accuracy: 0.7388 - loss: 0.5635 - val_accuracy: 0.7353 - val_loss: 0.5164\n",
      "Epoch 8/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 895ms/step - accuracy: 0.7267 - loss: 0.5888 - val_accuracy: 0.7647 - val_loss: 0.5461\n",
      "Epoch 9/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810ms/step - accuracy: 0.7513 - loss: 0.5472"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 905ms/step - accuracy: 0.7517 - loss: 0.5476 - val_accuracy: 0.8235 - val_loss: 0.4674\n",
      "Epoch 10/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 883ms/step - accuracy: 0.7536 - loss: 0.5668 - val_accuracy: 0.7647 - val_loss: 0.5008\n",
      "Epoch 11/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 891ms/step - accuracy: 0.7402 - loss: 0.5593 - val_accuracy: 0.8235 - val_loss: 0.5233\n",
      "Epoch 12/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 897ms/step - accuracy: 0.7594 - loss: 0.5624 - val_accuracy: 0.8235 - val_loss: 0.4612\n",
      "Epoch 13/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 896ms/step - accuracy: 0.7856 - loss: 0.5475 - val_accuracy: 0.8235 - val_loss: 0.4537\n",
      "Epoch 14/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 897ms/step - accuracy: 0.7534 - loss: 0.5520 - val_accuracy: 0.7941 - val_loss: 0.5090\n",
      "Epoch 15/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 923ms/step - accuracy: 0.7690 - loss: 0.5411 - val_accuracy: 0.7941 - val_loss: 0.4681\n",
      "Epoch 16/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.7642 - loss: 0.5356 - val_accuracy: 0.7941 - val_loss: 0.4854\n",
      "Epoch 17/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.8003 - loss: 0.5154 - val_accuracy: 0.7941 - val_loss: 0.4368\n",
      "Epoch 18/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.7498 - loss: 0.5454 - val_accuracy: 0.7941 - val_loss: 0.4524\n",
      "Epoch 19/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 893ms/step - accuracy: 0.7876 - loss: 0.5145 - val_accuracy: 0.7941 - val_loss: 0.4672\n",
      "Epoch 20/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 892ms/step - accuracy: 0.7849 - loss: 0.5021 - val_accuracy: 0.7941 - val_loss: 0.4647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# إعداد Augmentation + تقسيم البيانات\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,  # 20% للـ validation\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# تحميل البيانات وتوحيد الحجم\n",
    "train_data = datagen.flow_from_directory(\n",
    "    '../data/1000 QR Images/train',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    '../data/1000 QR Images/val',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# تحميل MobileNetV2 بدون الطبقات العلوية\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "base_model.trainable = False  # تثبيت الطبقات أثناء أول تدريب\n",
    "\n",
    "# إضافة الطبقات العليا\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# تجميع النموذج\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# حفظ أفضل نموذج\n",
    "checkpoint = ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor=\"val_accuracy\", mode=\"max\")\n",
    "early = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# تدريب النموذج\n",
    "model.fit(train_data, validation_data=val_data, epochs=20, callbacks=[checkpoint, early])\n",
    "\n",
    "# حفظ النموذج النهائي\n",
    "model.save(\"qr_spoofing_detector.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ca1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "Spoofing\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "img = image.load_img('../data/1000 QR Images/test/benign_qr_images_500/qr_497_benign_images.png', target_size=(256, 256))\n",
    "img_array = image.img_to_array(img) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "prediction = model.predict(img_array)\n",
    "print(\"Spoofing\" if prediction[0][0] > 0.5 else \"Legit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c3df5",
   "metadata": {},
   "source": [
    "using YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "464367e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt to 'yolo11m.pt'...\n",
      "Ultralytics 8.3.115  Python-3.13.2 torch-2.7.0+cpu CPU (Intel Core(TM) i7-8550U 1.80GHz)\n",
      "YOLO11m summary (fused): 125 layers, 20,091,712 parameters, 0 gradients, 68.0 GFLOPs\n",
      "\n",
      "Downloading https://ultralytics.com/images/bus.jpg to 'bus.jpg'...\n",
      "image 1/1 d:\\Eman Folder\\Training\\ML\\CLS Learning Solutio-Various Files-Microsoft Machine Learning new-20241028-108a\\project\\16-Apr 2025\\TrustSentinel-main\\models\\sentiment\\notebooks\\bus.jpg: 640x480 4 persons, 1 bus, 618.5ms\n",
      "Speed: 7.2ms preprocess, 618.5ms inference, 14.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
      " Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/38.8M [00:00<?, ?B/s]\n",
      "  0%|          | 128k/38.8M [00:00<00:31, 1.27MB/s]\n",
      "  1%|▏         | 512k/38.8M [00:00<00:14, 2.74MB/s]\n",
      "  3%|▎         | 1.00M/38.8M [00:00<00:10, 3.70MB/s]\n",
      "  4%|▍         | 1.50M/38.8M [00:00<00:09, 4.15MB/s]\n",
      "  5%|▌         | 2.00M/38.8M [00:00<00:08, 4.39MB/s]\n",
      "  6%|▋         | 2.50M/38.8M [00:00<00:08, 4.52MB/s]\n",
      "  8%|▊         | 3.00M/38.8M [00:00<00:08, 4.61MB/s]\n",
      "  9%|▉         | 3.50M/38.8M [00:00<00:07, 4.66MB/s]\n",
      " 10%|█         | 4.00M/38.8M [00:00<00:07, 4.71MB/s]\n",
      " 12%|█▏        | 4.50M/38.8M [00:01<00:07, 4.73MB/s]\n",
      " 13%|█▎        | 5.00M/38.8M [00:01<00:07, 4.75MB/s]\n",
      " 14%|█▍        | 5.50M/38.8M [00:01<00:07, 4.76MB/s]\n",
      " 15%|█▌        | 6.00M/38.8M [00:01<00:07, 4.77MB/s]\n",
      " 17%|█▋        | 6.50M/38.8M [00:01<00:07, 4.77MB/s]\n",
      " 18%|█▊        | 7.00M/38.8M [00:01<00:06, 4.78MB/s]\n",
      " 19%|█▉        | 7.50M/38.8M [00:01<00:06, 4.78MB/s]\n",
      " 21%|██        | 8.00M/38.8M [00:01<00:06, 4.79MB/s]\n",
      " 22%|██▏       | 8.50M/38.8M [00:01<00:06, 4.79MB/s]\n",
      " 23%|██▎       | 9.00M/38.8M [00:02<00:06, 4.79MB/s]\n",
      " 24%|██▍       | 9.50M/38.8M [00:02<00:06, 4.79MB/s]\n",
      " 26%|██▌       | 10.0M/38.8M [00:02<00:06, 4.80MB/s]\n",
      " 27%|██▋       | 10.5M/38.8M [00:02<00:06, 4.79MB/s]\n",
      " 28%|██▊       | 11.0M/38.8M [00:02<00:06, 4.79MB/s]\n",
      " 30%|██▉       | 11.5M/38.8M [00:02<00:05, 4.79MB/s]\n",
      " 31%|███       | 12.0M/38.8M [00:02<00:05, 4.79MB/s]\n",
      " 32%|███▏      | 12.5M/38.8M [00:02<00:05, 4.77MB/s]\n",
      " 34%|███▎      | 13.0M/38.8M [00:02<00:05, 4.78MB/s]\n",
      " 35%|███▍      | 13.5M/38.8M [00:03<00:05, 4.80MB/s]\n",
      " 36%|███▌      | 14.0M/38.8M [00:03<00:05, 4.80MB/s]\n",
      " 37%|███▋      | 14.5M/38.8M [00:03<00:05, 4.80MB/s]\n",
      " 39%|███▊      | 15.0M/38.8M [00:03<00:05, 4.79MB/s]\n",
      " 40%|███▉      | 15.5M/38.8M [00:03<00:05, 4.79MB/s]\n",
      " 41%|████      | 16.0M/38.8M [00:03<00:04, 4.78MB/s]\n",
      " 43%|████▎     | 16.5M/38.8M [00:03<00:04, 4.80MB/s]\n",
      " 44%|████▍     | 17.0M/38.8M [00:03<00:04, 4.79MB/s]\n",
      " 45%|████▌     | 17.5M/38.8M [00:03<00:04, 4.78MB/s]\n",
      " 46%|████▋     | 18.0M/38.8M [00:04<00:04, 4.79MB/s]\n",
      " 48%|████▊     | 18.5M/38.8M [00:04<00:04, 4.78MB/s]\n",
      " 49%|████▉     | 19.0M/38.8M [00:04<00:04, 4.78MB/s]\n",
      " 50%|█████     | 19.5M/38.8M [00:04<00:04, 4.78MB/s]\n",
      " 52%|█████▏    | 20.0M/38.8M [00:04<00:04, 4.78MB/s]\n",
      " 53%|█████▎    | 20.5M/38.8M [00:04<00:04, 4.73MB/s]\n",
      " 54%|█████▍    | 21.0M/38.8M [00:04<00:03, 4.79MB/s]\n",
      " 55%|█████▌    | 21.5M/38.8M [00:04<00:03, 4.64MB/s]\n",
      " 57%|█████▋    | 22.0M/38.8M [00:04<00:03, 4.76MB/s]\n",
      " 58%|█████▊    | 22.5M/38.8M [00:05<00:03, 4.87MB/s]\n",
      " 59%|█████▉    | 23.0M/38.8M [00:05<00:03, 4.84MB/s]\n",
      " 61%|██████    | 23.5M/38.8M [00:05<00:03, 4.83MB/s]\n",
      " 62%|██████▏   | 24.0M/38.8M [00:05<00:03, 4.82MB/s]\n",
      " 63%|██████▎   | 24.5M/38.8M [00:05<00:03, 4.81MB/s]\n",
      " 64%|██████▍   | 25.0M/38.8M [00:05<00:03, 4.81MB/s]\n",
      " 66%|██████▌   | 25.5M/38.8M [00:05<00:02, 4.80MB/s]\n",
      " 67%|██████▋   | 26.0M/38.8M [00:05<00:02, 4.80MB/s]\n",
      " 68%|██████▊   | 26.5M/38.8M [00:05<00:02, 4.80MB/s]\n",
      " 70%|██████▉   | 27.0M/38.8M [00:06<00:02, 4.80MB/s]\n",
      " 71%|███████   | 27.5M/38.8M [00:06<00:03, 3.34MB/s]\n",
      " 74%|███████▍  | 28.8M/38.8M [00:06<00:02, 5.25MB/s]\n",
      " 76%|███████▌  | 29.4M/38.8M [00:06<00:01, 5.12MB/s]\n",
      " 77%|███████▋  | 30.0M/38.8M [00:06<00:01, 5.02MB/s]\n",
      " 79%|███████▉  | 30.6M/38.8M [00:06<00:01, 4.95MB/s]\n",
      " 81%|████████  | 31.2M/38.8M [00:06<00:01, 4.90MB/s]\n",
      " 82%|████████▏ | 31.8M/38.8M [00:07<00:01, 4.88MB/s]\n",
      " 83%|████████▎ | 32.2M/38.8M [00:07<00:01, 4.84MB/s]\n",
      " 84%|████████▍ | 32.8M/38.8M [00:07<00:01, 4.80MB/s]\n",
      " 86%|████████▌ | 33.2M/38.8M [00:07<00:01, 4.83MB/s]\n",
      " 87%|████████▋ | 33.8M/38.8M [00:07<00:01, 4.81MB/s]\n",
      " 88%|████████▊ | 34.2M/38.8M [00:07<00:00, 4.80MB/s]\n",
      " 90%|████████▉ | 34.8M/38.8M [00:07<00:00, 4.79MB/s]\n",
      " 91%|█████████ | 35.2M/38.8M [00:07<00:00, 4.81MB/s]\n",
      " 92%|█████████▏| 35.8M/38.8M [00:07<00:00, 4.80MB/s]\n",
      " 93%|█████████▎| 36.2M/38.8M [00:08<00:00, 4.79MB/s]\n",
      " 95%|█████████▍| 36.8M/38.8M [00:08<00:00, 4.78MB/s]\n",
      " 96%|█████████▌| 37.2M/38.8M [00:08<00:00, 4.78MB/s]\n",
      " 97%|█████████▋| 37.8M/38.8M [00:08<00:00, 4.80MB/s]\n",
      " 99%|█████████▊| 38.2M/38.8M [00:08<00:00, 4.79MB/s]\n",
      "100%|█████████▉| 38.8M/38.8M [00:08<00:00, 4.79MB/s]\n",
      "100%|██████████| 38.8M/38.8M [00:08<00:00, 4.73MB/s]\n",
      "\n",
      "  0%|          | 0.00/134k [00:00<?, ?B/s]\n",
      " 95%|█████████▌| 128k/134k [00:00<00:00, 1.27MB/s]\n",
      "100%|██████████| 134k/134k [00:00<00:00, 1.33MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Predict using a pretrained YOLO model (e.g., YOLO11n) on an image\n",
    "!yolo predict model=yolo11m.pt source='https://ultralytics.com/images/bus.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ace846c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset ready for YOLO training!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# تعديل المسار حسب مكان مجلد الصور\n",
    "ORIGINAL_FOLDER = \"../data/1000 QR Images\"\n",
    "NEW_FOLDER = \"qr_dataset\"\n",
    "\n",
    "# إنشاء المجلدات الجديدة\n",
    "for split in ['train', 'val', 'test']:\n",
    "    os.makedirs(f\"{NEW_FOLDER}/images/{split}\", exist_ok=True)\n",
    "    os.makedirs(f\"{NEW_FOLDER}/labels/{split}\", exist_ok=True)\n",
    "\n",
    "# قراءة كل الصور\n",
    "all_images = [f for f in os.listdir(ORIGINAL_FOLDER) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "random.shuffle(all_images)\n",
    "\n",
    "# تقسيم البيانات\n",
    "total = len(all_images)\n",
    "train_split = int(total * 0.7)\n",
    "val_split = int(total * 0.9)\n",
    "\n",
    "splits = {\n",
    "    'train': all_images[:train_split],\n",
    "    'val': all_images[train_split:val_split],\n",
    "    'test': all_images[val_split:]\n",
    "}\n",
    "\n",
    "# وظيفة لكتابة ملف label مؤقت\n",
    "def write_label(filename, label, split):\n",
    "    label_path = os.path.join(NEW_FOLDER, \"labels\", split, Path(filename).stem + \".txt\")\n",
    "    with open(label_path, \"w\") as f:\n",
    "        f.write(f\"{label} 0.5 0.5 1 1\\n\")  # تنسيق YOLO: class cx cy w h (full image)\n",
    "\n",
    "# نقل الصور وكتابة التصنيفات\n",
    "for split, images in splits.items():\n",
    "    for img in images:\n",
    "        label = 1 if 'mal' in img.lower() else 0\n",
    "        src = os.path.join(ORIGINAL_FOLDER, img)\n",
    "        dst = os.path.join(NEW_FOLDER, \"images\", split, img)\n",
    "        shutil.copy2(src, dst)\n",
    "        write_label(img, label, split)\n",
    "\n",
    "# إنشاء ملف data.yaml\n",
    "data_yaml = {\n",
    "    'train': f\"{NEW_FOLDER}/images/train\",\n",
    "    'val': f\"{NEW_FOLDER}/images/val\",\n",
    "    'test': f\"{NEW_FOLDER}/images/test\",\n",
    "    'nc': 2,\n",
    "    'names': ['legit', 'malicious']\n",
    "}\n",
    "\n",
    "with open(f\"{NEW_FOLDER}/data.yaml\", \"w\") as f:\n",
    "    yaml.dump(data_yaml, f)\n",
    "\n",
    "print(\"✅ Dataset ready for YOLO training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d7f0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.115  Python-3.13.2 torch-2.7.0+cpu CPU (Intel Core(TM) i7-8550U 1.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11l.pt, data=qr_dataset/data.yaml, epochs=20, time=None, patience=100, batch=16, imgsz=256, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train2\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2    756736  ultralytics.nn.modules.block.C3k2            [1024, 256, 2, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   2365440  ultralytics.nn.modules.block.C3k2            [768, 512, 2, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 23        [16, 19, 22]  1   1412566  ultralytics.nn.modules.head.Detect           [2, [256, 512, 512]]          \n",
      "YOLO11l summary: 357 layers, 25,312,022 parameters, 25,312,006 gradients, 87.3 GFLOPs\n",
      "\n",
      "Transferred 1009/1015 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.60.7 ms, read: 0.10.1 MB/s, size: 0.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Eman Folder\\Training\\ML\\CLS Learning Solutio-Various Files-Microsoft Machine Learning new-20241028-108a\\project\\16-Apr 2025\\TrustSentinel-main\\models\\sentiment\\notebooks\\qr_dataset\\labels\\train.cache... 0 images, 816 backgrounds, 0 corrupt: 100%|██████████| 816/816 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING No labels found in D:\\Eman Folder\\Training\\ML\\CLS Learning Solutio-Various Files-Microsoft Machine Learning new-20241028-108a\\project\\16-Apr 2025\\TrustSentinel-main\\models\\sentiment\\notebooks\\qr_dataset\\labels\\train.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.30.0 ms, read: 0.00.0 MB/s, size: 0.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\Eman\\anaconda3\\envs\\condaCV\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Eman Folder\\Training\\ML\\CLS Learning Solutio-Various Files-Microsoft Machine Learning new-20241028-108a\\project\\16-Apr 2025\\TrustSentinel-main\\models\\sentiment\\notebooks\\qr_dataset\\labels\\val.cache... 0 images, 176 backgrounds, 0 corrupt: 100%|██████████| 176/176 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING No labels found in D:\\Eman Folder\\Training\\ML\\CLS Learning Solutio-Various Files-Microsoft Machine Learning new-20241028-108a\\project\\16-Apr 2025\\TrustSentinel-main\\models\\sentiment\\notebooks\\qr_dataset\\labels\\val.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\Eman\\anaconda3\\envs\\condaCV\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train2\\labels.jpg... \n",
      "WARNING zero-size array to reduction operation maximum which has no identity\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 167 weight(decay=0.0), 174 weight(decay=0.0005), 173 bias(decay=0.0)\n",
      "Image sizes 256 train, 256 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train2\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20         0G          0      9.674          0          0        256: 100%|██████████| 51/51 [11:49<00:00, 13.91s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [02:13<00:00, 22.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        176          0          0          0          0          0\n",
      "WARNING no labels found in detect set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\Eman\\anaconda3\\envs\\condaCV\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:655: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
      "c:\\Users\\Eman\\anaconda3\\envs\\condaCV\\Lib\\site-packages\\numpy\\_core\\_methods.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20         0G          0     0.1989          0          0        256:  76%|███████▋  | 39/51 [15:25<04:44, 23.74s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[32m      3\u001b[39m model = YOLO(\u001b[33m\"\u001b[39m\u001b[33myolo11l.pt\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# أو yolov8s.yaml لو الجهاز قوي\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mqr_dataset/data.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eman\\anaconda3\\envs\\condaCV\\Lib\\site-packages\\ultralytics\\engine\\model.py:790\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    787\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    789\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    792\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eman\\anaconda3\\envs\\condaCV\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:210\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    207\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eman\\anaconda3\\envs\\condaCV\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:393\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.tloss = (\n\u001b[32m    389\u001b[39m         (\u001b[38;5;28mself\u001b[39m.tloss * i + \u001b[38;5;28mself\u001b[39m.loss_items) / (i + \u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss_items\n\u001b[32m    390\u001b[39m     )\n\u001b[32m    392\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ni - last_opt_step >= \u001b[38;5;28mself\u001b[39m.accumulate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eman\\anaconda3\\envs\\condaCV\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eman\\anaconda3\\envs\\condaCV\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eman\\anaconda3\\envs\\condaCV\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo11l.pt\")  # أو yolov8s.yaml لو الجهاز قوي\n",
    "model.train(data=\"qr_dataset/data.yaml\", epochs=20, imgsz=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
